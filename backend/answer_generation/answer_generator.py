"""
RAG Answer Generation Module

This module provides grounded answer generation using OpenRouter API.
It takes user queries and retrieved context chunks to generate accurate,
contextually-relevant answers without hallucination.
"""
import logging
from typing import List, Optional, Dict, Any
from dataclasses import dataclass


@dataclass
class GeneratedAnswer:
    """
    Represents the final answer generated by the system
    """
    answer_text: str
    confidence_score: float = 0.5
    source_citations: List[str] = None
    hallucination_detected: bool = False

    def __post_init__(self):
        if self.source_citations is None:
            self.source_citations = []


@dataclass
class QueryWithContext:
    """
    Represents a user query combined with retrieved context for answer generation
    """
    query: str
    context_chunks: List[str]
    retrieved_sources: List[str] = None

    def __post_init__(self):
        if self.retrieved_sources is None:
            self.retrieved_sources = []


# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


import time

def generate_answer(query: str, context_chunks: List[str], sources: List[str] = None) -> GeneratedAnswer:
    """
    Main function to generate an answer based on query and context

    Args:
        query: User query text
        context_chunks: List of context chunk strings from retrieval
        sources: Optional list of source URLs for context chunks

    Returns:
        GeneratedAnswer object with answer text and metadata
    """
    start_time = time.time()

    # Sanitize inputs
    from .utils import sanitize_input, validate_query_format, validate_context_chunks_format
    sanitized_query = sanitize_input(query)
    logger.info(f"Generating answer for query: {sanitized_query[:50]}...")

    # Validate inputs
    if not validate_query_format(sanitized_query):
        raise ValueError("Query format is invalid")

    if not validate_context_chunks_format(context_chunks if context_chunks else []):
        if context_chunks:  # If context was provided but is invalid
            logger.warning("Context chunks format is invalid")
        else:
            logger.warning("No context provided for answer generation")

    # Handle empty context case
    if not context_chunks or not any(chunk.strip() for chunk in context_chunks):
        logger.warning("No valid context provided for answer generation")
        return GeneratedAnswer(
            answer_text="I cannot answer this question based on the provided context.",
            confidence_score=0.0,
            hallucination_detected=False
        )

    # Construct prompt using the prompt constructor
    from .prompt_constructor import construct_prompt
    prompt_start_time = time.time()
    prompt = construct_prompt(sanitized_query, context_chunks)
    prompt_time = time.time() - prompt_start_time
    logger.debug(f"Prompt construction took {prompt_time:.3f}s")

    # Call OpenRouter API to generate answer
    from .openrouter_client import call_openrouter_api
    api_start_time = time.time()
    response_text = call_openrouter_api(prompt)
    api_time = time.time() - api_start_time
    logger.debug(f"OpenRouter API call took {api_time:.3f}s")

    if response_text is None:
        # Handle API failure with fallback response
        total_time = time.time() - start_time
        logger.info(f"Answer generation completed with API failure in {total_time:.3f}s")
        return GeneratedAnswer(
            answer_text="I'm sorry, but I couldn't generate an answer at this time. Please try again later.",
            confidence_score=0.0,
            source_citations=sources or [],
            hallucination_detected=False
        )

    # Apply post-processing to the response
    from .utils import detect_hallucination, remove_unsupported_content, calculate_confidence_score, format_source_citations
    processing_start_time = time.time()
    processed_answer = remove_unsupported_content(response_text)

    # Validate that the answer is grounded in the provided context
    from .utils import AnswerValidator
    is_properly_grounded = AnswerValidator.validate_answer_grounding(processed_answer, context_chunks)

    if not is_properly_grounded:
        logger.warning("Generated answer may not be properly grounded in context")

    has_hallucination = detect_hallucination(processed_answer, context_chunks)
    confidence = calculate_confidence_score(context_chunks, processed_answer)
    formatted_sources = format_source_citations(sources or [], processed_answer)
    processing_time = time.time() - processing_start_time
    logger.debug(f"Response processing took {processing_time:.3f}s")

    total_time = time.time() - start_time
    logger.info(f"Answer generation completed successfully in {total_time:.3f}s (prompt: {prompt_time:.3f}s, api: {api_time:.3f}s, processing: {processing_time:.3f}s)")

    return GeneratedAnswer(
        answer_text=processed_answer,
        confidence_score=confidence,
        source_citations=formatted_sources,
        hallucination_detected=has_hallucination
    )


def generate_answer_with_sources(query: str, context_chunks: List[str], sources: List[str] = None) -> GeneratedAnswer:
    """
    Generate answer with explicit source tracking
    """
    return generate_answer(query, context_chunks, sources)


class AnswerGenerator:
    """
    Main class for answer generation with configurable parameters
    """

    def __init__(self):
        """Initialize the answer generator with configuration"""
        from .config import load_config
        self.config = load_config()
        self.logger = logging.getLogger(__name__)

    def generate(self, query: str, context_chunks: List[str], sources: List[str] = None) -> GeneratedAnswer:
        """
        Generate answer using the instance configuration
        """
        return generate_answer(query, context_chunks, sources)

    def generate_with_progress_tracking(self, query: str, context_chunks: List[str], sources: List[str] = None) -> GeneratedAnswer:
        """
        Generate answer with progress tracking and reporting
        """
        self.logger.info("Starting answer generation process")
        print("[PROGRESS] 10% - Starting answer generation process")

        # Step 1: Input validation and assembly
        self.logger.debug("Step 1: Validating and assembling inputs")
        print("[PROGRESS] 20% - Validating query and context inputs")
        from .utils import validate_query_format, validate_context_chunks_format
        if not validate_query_format(query):
            raise ValueError("Invalid query format")

        if not validate_context_chunks_format(context_chunks if context_chunks else []):
            self.logger.warning("Context chunks format validation failed")

        # Step 2: Prompt construction
        self.logger.debug("Step 2: Constructing prompt with context and query")
        print("[PROGRESS] 40% - Constructing prompt with context and query")
        from .prompt_constructor import construct_prompt
        prompt = construct_prompt(query, context_chunks)

        # Step 3: API call
        self.logger.debug("Step 3: Calling OpenRouter API")
        print("[PROGRESS] 60% - Calling OpenRouter API for answer generation")
        from .openrouter_client import call_openrouter_api
        response_text = call_openrouter_api(prompt)

        if response_text is None:
            self.logger.error("OpenRouter API call failed")
            print("[PROGRESS] 90% - API call failed, returning fallback response")
            return GeneratedAnswer(
                answer_text="I'm sorry, but I couldn't generate an answer at this time. Please try again later.",
                confidence_score=0.0,
                source_citations=sources or [],
                hallucination_detected=False
            )

        # Step 4: Response processing
        self.logger.debug("Step 4: Processing API response")
        print("[PROGRESS] 80% - Processing API response and validating answer")
        from .utils import detect_hallucination, remove_unsupported_content, calculate_confidence_score, format_source_citations
        processed_answer = remove_unsupported_content(response_text)
        has_hallucination = detect_hallucination(processed_answer, context_chunks)
        confidence = calculate_confidence_score(context_chunks, processed_answer)
        formatted_sources = format_source_citations(sources or [], processed_answer)

        self.logger.info("Answer generation process completed successfully")
        print("[PROGRESS] 100% - Answer generation completed successfully")

        return GeneratedAnswer(
            answer_text=processed_answer,
            confidence_score=confidence,
            source_citations=formatted_sources,
            hallucination_detected=has_hallucination
        )


def validate_all_requirements() -> bool:
    """
    Final validation to confirm all requirements are satisfied
    """
    logger.info("Running final validation of all requirements")

    # Import necessary modules
    from .config import Config
    from .utils import AnswerValidator

    # Check configuration is loaded
    try:
        config = Config()
        assert config.OPENROUTER_API_KEY is not None, "API key should be configured"
        assert config.OPENROUTER_BASE_URL is not None, "Base URL should be configured"
        assert config.OPENROUTER_MODEL is not None, "Model should be configured"
        logger.info("✓ Configuration validation passed")
    except Exception as e:
        logger.error(f"✗ Configuration validation failed: {e}")
        return False

    # Check that all core functions exist
    try:
        assert callable(generate_answer), "generate_answer function should exist"
        assert callable(generate_answer_with_sources), "generate_answer_with_sources function should exist"
        logger.info("✓ Function existence validation passed")
    except Exception as e:
        logger.error(f"✗ Function existence validation failed: {e}")
        return False

    # Test basic functionality
    try:
        test_query = "What are the key features?"
        test_context = [
            "The system provides comprehensive RAG functionality.",
            "Key features include context retrieval and answer generation."
        ]

        # Test normal generation
        answer = generate_answer(test_query, test_context)
        assert answer.answer_text is not None, "Answer text should not be None"
        assert isinstance(answer.confidence_score, float), "Confidence score should be float"
        assert 0.0 <= answer.confidence_score <= 1.0, "Confidence score should be between 0.0 and 1.0"
        assert isinstance(answer.source_citations, list), "Source citations should be a list"
        assert isinstance(answer.hallucination_detected, bool), "Hallucination detection should be boolean"

        # Test with sources
        answer_with_sources = generate_answer_with_sources(test_query, test_context, ["https://example.com"])
        assert answer_with_sources.source_citations is not None, "Source citations should be populated"

        logger.info("✓ Basic functionality validation passed")
    except Exception as e:
        logger.error(f"✗ Basic functionality validation failed: {e}")
        return False

    # Test AnswerGenerator class
    try:
        generator = AnswerGenerator()
        assert hasattr(generator, 'generate'), "AnswerGenerator should have generate method"
        assert hasattr(generator, 'generate_with_progress_tracking'), "AnswerGenerator should have generate_with_progress_tracking method"

        # Test class-based generation
        class_answer = generator.generate(test_query, test_context)
        assert class_answer.answer_text is not None, "Class-generated answer should not be None"

        logger.info("✓ AnswerGenerator class validation passed")
    except Exception as e:
        logger.error(f"✗ AnswerGenerator class validation failed: {e}")
        return False

    # Test error handling
    try:
        # Test with empty context
        empty_answer = generate_answer(test_query, [])
        assert empty_answer.answer_text is not None, "Should handle empty context gracefully"

        # Test with empty query
        try:
            generate_answer("", test_context)
            logger.warning("Empty query validation: function did not raise expected error")
        except ValueError:
            # This is expected
            pass

        logger.info("✓ Error handling validation passed")
    except Exception as e:
        logger.error(f"✗ Error handling validation failed: {e}")
        return False

    logger.info("✓ All requirements validation passed successfully")
    return True


# Example usage and testing
if __name__ == "__main__":
    # Run final validation
    print("Running final validation of all requirements...")
    validation_passed = validate_all_requirements()

    if validation_passed:
        print("\nSUCCESS: All requirements validated successfully!")
    else:
        print("\nERROR: Some requirements failed validation")
        exit(1)

    # Example usage
    query = "What are the key features?"
    context_chunks = [
        "The system provides comprehensive RAG functionality.",
        "Key features include context retrieval and answer generation."
    ]

    answer = generate_answer(query, context_chunks)
    print(f"\nAnswer: {answer.answer_text}")
    print(f"Confidence: {answer.confidence_score}")
    print(f"Sources: {answer.source_citations}")
    print(f"Hallucination detected: {answer.hallucination_detected}")