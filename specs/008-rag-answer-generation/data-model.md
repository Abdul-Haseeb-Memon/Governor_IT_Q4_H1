# Data Model: RAG Answer Generation using OpenRouter

**Feature**: RAG Answer Generation
**Date**: 2025-12-25

## Core Entities

### QueryWithContext
**Description**: Represents a user query combined with retrieved context for answer generation

**Fields**:
- `query`: str
  - **Type**: String
  - **Required**: Yes
  - **Validation**: Must not be empty, length 1-1000 characters
  - **Description**: The original user query text

- `context_chunks`: List[str]
  - **Type**: Array of strings
  - **Required**: Yes
  - **Validation**: Must contain at least 1 chunk, each chunk 1-5000 characters
  - **Description**: Retrieved context chunks from Spec-2 retrieval

- `retrieved_sources`: List[str]
  - **Type**: Array of strings
  - **Required**: No
  - **Default**: Empty list
  - **Description**: Source URLs for the context chunks

### GeneratedAnswer
**Description**: Represents the final answer generated by the system

**Fields**:
- `answer_text`: str
  - **Type**: String
  - **Required**: Yes
  - **Validation**: Must not be empty
  - **Description**: The generated answer text grounded in context

- `confidence_score`: float
  - **Type**: Float
  - **Required**: No
  - **Default**: 0.5
  - **Range**: 0.0 to 1.0
  - **Description**: Confidence in the grounding of the answer

- `source_citations`: List[str]
  - **Type**: Array of strings
  - **Required**: No
  - **Default**: Empty list
  - **Description**: List of sources used in generating the answer

- `hallucination_detected`: bool
  - **Type**: Boolean
  - **Required**: No
  - **Default**: False
  - **Description**: Flag indicating if potential hallucination was detected

## API Contracts

### Function: generate_answer(query: str, context_chunks: List[str], sources: List[str] = None) -> GeneratedAnswer
**Input**:
- query: User query text (1-1000 characters)
- context_chunks: List of context chunk strings
- sources: Optional list of source URLs for context chunks

**Output**:
- GeneratedAnswer object with answer_text and metadata

**Errors**:
- ValueError if query is invalid
- Exception if OpenRouter API call fails
- Safe fallback if context is insufficient

### Function: construct_prompt(query: str, context_chunks: List[str]) -> str
**Input**:
- query: User query text
- context_chunks: List of context chunk strings

**Output**:
- Formatted prompt string ready for LLM

**Errors**:
- ValueError if inputs are invalid

### Function: validate_context(context_chunks: List[str]) -> bool
**Input**:
- context_chunks: List of context chunk strings

**Output**:
- Boolean indicating if context is sufficient for answering

**Errors**:
- None, always returns a boolean

## Data Flow

### Answer Generation Flow
1. **Input**: User query and context chunks from Spec-2
2. **Processing**: Prompt construction with grounding instructions
3. **Inference**: OpenRouter API call with deterministic parameters
4. **Output**: GeneratedAnswer with grounded response and metadata

### Data Validation Rules
- Query must be between 1 and 1000 characters
- Context chunks must contain at least 1 chunk
- Each context chunk must be between 1 and 5000 characters
- Generated answer must not be empty
- Confidence score must be between 0.0 and 1.0
- Source citations must match provided context sources

## Integration with Spec-2 Data Model

The answer generation layer expects the following from Spec-2:

- `context_chunks`: List of content chunk texts
- `source_urls`: Corresponding source URLs for each chunk
- `relevance_scores`: Optional relevance scores for each chunk (for prioritization)

This ensures seamless integration with the retrieval pipeline.